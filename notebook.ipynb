{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training deep RL agents to achieve 1.6% net returns daily\n",
    "\n",
    "**While information here is presented in good faith, it does not constitue financial advice or replace a qualified financial advisor. This notebook is intended for educational purposes only. Use at your own risk.**\n",
    "\n",
    "If you're just looking for directions to deploy your own automated trading bot, please skip to the final section in this tutorial.\n",
    "\n",
    "Automated stock trading is a very real world problem that accenuates the challanges and limitations of supervised learning, the simulation-reality gap, continual learning, and hyperparameter tuning. Unlike the real market -- which is highly chaotic -- datasets do not respond to trading behavior. Reinforcement learning simulators too can only provide samples of synthetic market behavior, and even this does not accurately reflect the behavior of human investors. The automated trading problem is also nonstationary -- the market can change at any time and agents adapted to previous trends might not be able to operate competitively under the new market conditions. Even continual learning powered solutions may eventually degenerate and require manual intervention, re-tuning, or architecture re-design. Developing profitable trading agents is therefore an excellent way to jump into the bleeding edge of machine learning.\n",
    "\n",
    "This tutorial walks through training and deploying a high frequency (minute-level) deep reinforcement learning stock trading agent using tensorflow and alpaca. We'll start by overviewing the general problem. Then we'll look at the Alpaca API and how it can be used to trade stocks. Next, we'll propose a few candidate agent and training architectures to experiment with. Then, we will build a training pipeline and run it on historical data. After analysis and hyperparameter tuning, we'll test-deploy our agents on live paper-trading markets. Finally, we'll set up daily email notifications, schedule automated GCP deployments during market hours, and let our agents loose in the wild (the IEX exchange)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "Stock trading basically aims to make a profit by buying low and selling high. Formally, given a sequence of average stock prices (vwap) $p \\in \\mathbb{R}^{T \\times N_s}$, portfolio holdings $h \\in \\mathbb{Z}^{T \\times N_s}$, cash $c \\in \\mathbb{R}^T$, and other per-stock and market-level trading signals $s_{indv} \\in \\mathbb{R}^{T \\times (N_s N_{indv\\ sig})}$ and $s_{mkt} \\in \\mathbb{R}^{T \\times N_{mkt\\ sig}}$ respectively, the trading agent must execute trading decisions (bid/hold/offer choice and count $d \\in \\mathbb{Z}^{T \\times N_s}$, max bid price $p^{bid} \\in \\mathbb{R}^{T \\times N_s}$, and min offer price $p^{offer} \\in \\mathbb{Z}^{T \\times N_s}$) for each time step $t \\in [0,T]$ for $N_s$ stocks such that net worth (reward) $r_t = c_t + \\sum_{\\forall\\ i \\in \\text{stocks}} h^i_t p^i_t$ is maximized. Observations and actions are concatenated along the stock dimension for each timestep with market-level variables structured in a separate flat tensor as follows: $o = ( o_{mkt}, o_{indv} )$, $o_{indv} = [p; h; s_{indv}; d; p^{bid}; p^{offer}] \\in \\mathbb{N/R}^{T \\times N_s \\times \\cdots}$, $o_{mkt} = [c; s_{mkt}; r]  \\in \\mathbb{N/R}^{T \\times \\cdots}$, and $a = [d, p^{bid}, p^{offer}] \\in \\mathbb{N/R}^{T \\times N_s \\times 3}$.\n",
    "\n",
    "The bid/hold/offer choice and count $d^i_t$ is interpreted in three cases:\n",
    "- $d^i_t = 0$ results in stock $i$ being held at time $t$,\n",
    "- $d^i_t < 0$ results in $\\max \\{ |d^i_t| , h^i_t \\}$ shares of stock $i$ being offered for sale at a price of $p^{offer,i}_t$, and\n",
    "- $d^i_t > 0$ results in $d^i_t$ shares of stock $i$ (or as many as can be afforded) being bid on for purchase at a price of $p^{bid,i}_t$.\n",
    "\n",
    "Since the agent makes purchase bids and market offers, its decisions do not necesarily result in a transaction depending on the market's condition. For example, if the agent has $h^i_t = 0$ and $d^i_t = -1$ (i.e., it wants to sell one share of stock $i$), no shares will actually be offered for sale.\n",
    "\n",
    "Additional market signals used to drive the agent's behavior are:\n",
    "- Share price volatility\n",
    "- Market volatility\n",
    "- Sharpe ratio\n",
    "- Market sharpe ratio\n",
    "- Share moving average\n",
    "- Market moving average\n",
    "- Forecasted share price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO. I should make the dataset and environment here so we can see exactly what an observation or action looks like. Actually, I should split the above cell and put this one in between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Succintly, we can now describe a primary optimization objective of $\\pi^{*} = \\max_\\pi \\sum \\lambda^t r_t$ where $\\lambda^t \\in [0,1]$ is the reward discount. Values of $\\lambda$ near 0 result in immediate payoff being maximized, while values near 1 result in long-term payoff being maximized.\n",
    "\n",
    "Reinforcement learning only provides a single feedback signal at each timestep. Therefore, training is slow and requires a large number of training iterations. Research in multitask learning has shown that, in many cases, augmenting a model's training paradigm with auxillary objectives results in faster convergence and superior performance -- even when these auxillary objectives outnumber the final task objective in terms of number of training examples seen. I'm going to make the assumption that training the base layers of the trading agent to perform forward and backward autoregressive modeling of $o$ for each timestep should encode important information about market dynamics into the weights of those layers. Self-supervised autoregressive training epochs will be probabblistically interspersed by portfolio-maximization reinforcement learning episodes with the ratio of self-supervised to reinforcement learning epochs decreasing as training epochs progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**\n",
    "\n",
    "sequence transformations\n",
    "- vanilla\n",
    "- forier features\n",
    "- multiscale view\n",
    "\n",
    "base layers\n",
    "- {attention,max,ave} pooling over all stocks\n",
    "- {CNN,RNN,LSTM,linformer} over sequence\n",
    "\n",
    "multiple agents\n",
    "- comptetive vs. cooperative\n",
    "- agents can communicate with internal channels\n",
    "- agents observe each others actions\n",
    "\n",
    "decision output head\n",
    "- vanilla output unit for each stock\n",
    "- trainable weighted 0th, 1st, and 2nd order integrator\n",
    "- powers of two to Nmax output units for each stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 17:21:37.082644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64::/home/jacob/.mujoco/mujoco200/bin:/usr/local/pulse/extra/usr/lib/x86_64-linux-gnu/\n",
      "2022-01-06 17:21:37.082693: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensroflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_314867/4092811771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensroflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfkm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensroflow'"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import getpass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow.keras.models as tfkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca Trading API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Alpaca Trading Bot"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "605fe966a75bc2c3dfa708e269323e6491854b30a36f4e77953579e94649bfba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
